# -*- coding: utf-8 -*-
"""36106-25AU-AT2-25589351-experiment-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQt7WLOfaoAsKUZyj87TlrVdlx3w1j6o

# **Experiment Notebook**

---
## 0. Setup Environment

### 0.a Install Environment and Mandatory Packages
"""

# Do not modify this code
!pip install -q utstd

from utstd.folders import *
from utstd.ipyrenders import *

at = AtFolder(
    course_code=36106,
    assignment="AT2",
)
at.run()

"""### 0.b Disable Warnings Messages"""

# Do not modify this code
import warnings
warnings.simplefilter(action='ignore')

"""### 0.c Install Additional Packages

> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`
"""

# <Student to fill this section>

"""### 0.d Import Packages"""

# <Student to fill this section>
import pandas as pd
import altair as alt
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV

"""---
## A. Project Description

"""

# <Student to fill this section>
student_name = "Fatemeh Elyasifar"
student_id = "25589351"

# Do not modify this code
print_tile(size="h1", key='student_name', value=student_name)

# Do not modify this code
print_tile(size="h1", key='student_id', value=student_id)

print("Student Name:", student_name)
print("Student ID:", student_id)

# <Student to fill this section>
business_objective = """

The objective is to develop a reliable and interpretable machine learning model to predict student performance at the end of the semester
using academic, behavioral, and demographic data. The model aims to help university staff, student support officers, and academic advisors identify at-risk students (those with poor or average performance),
with a target F1-score and recall above 80%, enabling timely outreach and efficient allocation of support services.
The focus is on improving recall for underperforming students while maintaining balanced performance.
The project also identifies key predictors and includes feature tuning to enhance accuracy and relevance.

Accurate predictions help the university support underperforming students, improve outcomes, and reduce dropout rates.
Inaccurate results risk missing students in need or misusing limited resources.
Therefore, maintaining a strong balance between precision and recall is essential to ensure the model is both effective and trustworthy.
High recall is especially important to capture as many at-risk students as possible.
"""

# Do not modify this code
print_tile(size="h3", key='business_objective', value=business_objective)

print("Business Objective:", business_objective)

"""---
## B. Experiment Description
"""

# Do not modify this code
experiment_id = "2"
print_tile(size="h1", key='experiment_id', value=experiment_id)

print("Experiment ID:", experiment_id)

# <Student to fill this section>
experiment_hypothesis = """
Present the hypothesis you want to test, the question you want to answer or the insight you are seeking.
Explain the reasons why you think it is worthwhile considering it

Student performance can be predicted using a Decision Tree classifier trained on behavioral, academic, and demographic features.
This experiment aims to test whether a tree-based model can provide interpretable decision paths while maintaining competitive performance.

The hypothesis is that DecisionTreeClassifier can produce meaningful predictions with a simpler model structure and higher interpretability.
It will perform better than SVC, as it is less sensitive to irrelevant features and does not rely on a smaller, highly optimized feature set.
This makes it a strong candidate for supporting early identification of at-risk students while offering better transparency.
"""

# Do not modify this code
print_tile(size="h3", key='experiment_hypothesis', value=experiment_hypothesis)

print("Experiment Hypothesis:", experiment_hypothesis)

# <Student to fill this section>
experiment_expectations = """
Detail what will be the expected outcome of the experiment. If possible, estimate the goal you are expecting.
List the possible scenarios resulting from this experiment.

The Decision Tree Classifier is expected to outperform both the baseline and SVC models, with a weighted F1 score around 80 and recall near 85.
Its ability to handle the full feature set and ignore irrelevant features may lead to better overall performance.

Different hyperparameter settings (e.g., max_depth, min_samples_split, criterion) will be tested to optimize recall,
especially for the 'Poor' and 'average' category. The model’s interpretability is also a key benefit for practical use.

Possible scenarios include:
- Strong improvement over SVC and baseline, showing good feature interaction and accuracy.
- Similar performance to SVC but with better interpretability.
- Weaker results due to overfitting or imbalance, indicating a need for ensemble methods like Random Forest or Extra Trees.
"""

# Do not modify this code
print_tile(size="h3", key='experiment_expectations', value=experiment_expectations)

print("Experiment Expectations:", experiment_expectations)

"""---
## C. Data Understanding
"""

# Do not modify this code
# Load training data
try:
  X_train = pd.read_csv(at.folder_path / 'X_train.csv')
  y_train = pd.read_csv(at.folder_path / 'y_train.csv')

  X_val = pd.read_csv(at.folder_path / 'X_val.csv')
  y_val = pd.read_csv(at.folder_path / 'y_val.csv')

  X_test = pd.read_csv(at.folder_path / 'X_test.csv')
  y_test = pd.read_csv(at.folder_path / 'y_test.csv')
except Exception as e:
  print(e)

"""---
## D. Feature Selection

"""

# <Student to fill this section>

features_list = X_train.columns.tolist()

# <Student to fill this section>
feature_selection_explanations = """
Provide a rationale on why you are selected these features but also why you decided to remove other ones

These attributes offer a detailed perspective on the student's academic conditions, behaviors, and personal context,
and were selected to highlight the most relevant factors affecting student performance.
The selected features are well-suited for training a Decision Tree model, as it can handle a large number of variables
and identify meaningful patterns.
Less relevant features were removed in Experiment 0 based on their low impact on the target variable.
"""

# Do not modify this code
print_tile(size="h3", key='feature_selection_explanations', value=feature_selection_explanations)

print("Feature Selection Explanations:", feature_selection_explanations)

"""---
## G. Train Machine Learning Model

### G.1 Import Algorithm
"""

# <Student to fill this section>
from sklearn.tree import DecisionTreeClassifier

# <Student to fill this section>
algorithm_selection_explanations = """
Provide some explanations on why you believe this algorithm is a good fit

Decision Tree Classifier is a good fit for this experiment because it can model complex, non-linear relationships
and handle a wide range of numerical features effectively. It is robust to irrelevant or less important features
and can naturally capture interactions between variables.

Another key advantage is its high interpretability, which allows for clear explanations of prediction outcomes,
an important factor when identifying at-risk students in an educational setting.
Its ability to handle a large feature set also makes it well-suited for this dataset.
"""

# Do not modify this code
print_tile(size="h3", key='algorithm_selection_explanations', value=algorithm_selection_explanations)

print("Algorithm Selection Explanations:", algorithm_selection_explanations)

"""### G.2 Set Hyperparameters"""

# <Student to fill this section>
param_grid = {
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20],
    'criterion': ['gini', 'entropy'],
    'class_weight': ['balanced']
}

# <Student to fill this section>
hyperparameters_selection_explanations = """
Explain why you are tuning these hyperparameters

These hyperparameters are tuned to control the complexity and performance of the Decision Tree.
'max_depth' limits how deep the tree can grow to avoid overfitting, while 'min_samples_split' controls the minimum number of samples required to split a node.
'criterion' determines how the quality of a split is measured ('gini' or 'entropy'),
and 'class_weight' is set to 'balanced' to handle class imbalance in the target variable.
Tuning these values helps improve the model's generalization and recall, especially for the less frequent classes.
"""

# Do not modify this code
print_tile(size="h3", key='hyperparameters_selection_explanations', value=hyperparameters_selection_explanations)

print("Hyperparameters Selection Explanations:", hyperparameters_selection_explanations)

"""### G.3 Fit Model"""

# <Student to fill this section>
dt_model = DecisionTreeClassifier(random_state=42)

# Grid Search
grid_search_dt = GridSearchCV(dt_model, param_grid, cv=5, scoring='f1_weighted')
grid_search_dt.fit(X_train, y_train)

print("Best parameters:", grid_search_dt.best_params_)
print("Best Weighted F1 Score:", round(grid_search_dt.best_score_, 4))

results_df = pd.DataFrame(grid_search_dt.cv_results_)
results_df = results_df[[
    'mean_test_score', 'std_test_score', 'params'
]]
results_df = results_df.sort_values(by='mean_test_score', ascending=False).reset_index(drop=True)

# Display full params in the results DataFrame
pd.set_option('display.max_colwidth', None)

results_df

dt_model = DecisionTreeClassifier(class_weight= 'balanced', criterion= 'entropy', max_depth= 5, min_samples_split= 20, random_state=42)
dt_model.fit(X_train, y_train)

cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='f1_weighted')
print("Cross-Validated Weighted F1 Scores (Train Set):", cv_scores)
print("Mean CV Weighted F1 Score:", cv_scores.mean())

"""### G.4 Model Technical Performance"""

# <Student to fill this section>

y_preds = dt_model.predict(X_val)

print("DecisionTree Evaluation:\n")
print("\nRecall_score:", recall_score(y_val, y_preds, average='weighted'))
print("\nf1_score:", f1_score(y_val, y_preds, average='weighted'))
print("\nClassification Report:\n", classification_report(y_val, y_preds))
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_preds))

# <Student to fill this section>
model_performance_explanations = """
Provide some explanations on model performance

The Decision Tree model achieved a weighted F1 score of 0.90 in cross-validation and 0.94 on the validation set, with a recall of 0.94.
This indicates strong and consistent performance across data splits.

GridSearchCV was used to efficiently explore multiple hyperparameter combinations for the Decision Tree, as manual tuning would be impractical due to the many possible hyperparameter combinations.
Although the highest-scoring configuration was identified, a slightly lower-performing set (around 90% of the best score) was selected to reduce the risk of overfitting and improve generalization to unseen data.

The model shows excellent recall for the 'Poor' class (label 0), correctly identifying 96 out of 103 students, and strong recall for the 'Average' class (label 1),
identifying 47 out of 51 students, which is critical for early intervention.

Weighted F1 score was used due to class imbalance, ensuring balanced performance across all categories.
The confusion matrix shows minimal misclassifications.

Overall, the model outperforms SVC and demonstrates strong predictive ability;
however, since accuracy above 90% is rare in real-world business problems, there may be a risk of data leakage or overfitting in this case.
So the model may not be reliable and could fail when applied to real-world data.
"""

# Do not modify this code
print_tile(size="h3", key='model_performance_explanations', value=model_performance_explanations)

print("Model Performance Explanations:", model_performance_explanations)

"""### G.5 Business Impact from Current Model Performance

"""

# <Student to fill this section>

y_pred_final = dt_model.predict(X_test)

print("DecisionTree Evaluation:\n")
print("\nRecall_score:", recall_score(y_test, y_pred_final, average='weighted'))
print("\nf1_score:", f1_score(y_test, y_pred_final, average='weighted'))
print("\nClassification Report:\n", classification_report(y_test, y_pred_final))

ConfusionMatrixDisplay.from_estimator(dt_model, X_test, y_test, cmap='cividis')

# <Student to fill this section>
business_impacts_explanations = """
Interpret the results of the experiments related to the business objective set earlier. Estimate the impacts of the incorrect results for the business (some results may have more impact compared to others)

The Decision Tree model performed very well on unseen test data, with both weighted F1 score and recall around 94%.
It correctly identified most students across all performance levels, especially those in the 'Poor' and 'Average' categories, which are key for early support.
This can help the university take timely action and improve student outcomes.

Although the results are strong, the high accuracy may indicate possible data leakage or overfitting.
If that’s the case, the model might not perform as well on future data.
This could lead to missed opportunities for early intervention, affecting student success and overall institutional performance.
So, more testing and review are needed before deploying it in a real-world setting.
Also, the class distribution remains imbalanced, which may affect the model’s ability to generalize to new student groups.
"""

# Do not modify this code
print_tile(size="h3", key='business_impacts_explanations', value=business_impacts_explanations)

print("Business Impacts Explanations:", business_impacts_explanations)

"""## H. Experiment Outcomes"""

# <Student to fill this section>
experiment_outcome = "Hypothesis Confirmed" # Either 'Hypothesis Confirmed', 'Hypothesis Partially Confirmed' or 'Hypothesis Rejected'

# Do not modify this code
print_tile(size="h2", key='experiment_outcomes_explanations', value=experiment_outcome)

print("Experiment Outcome:", experiment_outcome)

# <Student to fill this section>
experiment_results_explanations = """
Reflect on the outcome of the experiment and list the new insights you gained from it. Provide rationale for pursuing more experimentation with the current approach or call out if you think it is a dead end.
Given the results achieved and the overall objective of the project, list the potential next steps and experiments. For each of them assess the expected uplift or gains and rank them accordingly. If the experiment achieved the required outcome for the business, recommend the steps to deploy this solution into production.

The best combination of hyperparameters was: class_weight='balanced', criterion='entropy', max_depth=5, and min_samples_split=20.
The Decision Tree model confirmed the initial hypothesis by achieving strong overall performance, outperforming both the baseline and SVC models, and accurately identifying the majority of at-risk students, including those in the 'Poor' and 'Average' categories.
However, the unusually high performance raises concerns about possible data leakage or overfitting, indicating the need for further validation.

Next steps include:

1. **Experiment with ensemble models (e.g., Extra Trees)**
   - Expected gain: Moderate to high (improved performance through multiple decision trees)
   - Priority: High

2. **Apply advanced sampling techniques (e.g., SMOTE)**
   - Expected gain: Moderate (helps address class imbalance and may reduce overfitting caused by underrepresented classes if applied correctly on training data)
   - Priority: Medium

If the model continues to perform well after these validations, it may be a strong candidate to support early academic intervention in real-world applications.
"""

# Do not modify this code
print_tile(size="h2", key='experiment_results_explanations', value=experiment_results_explanations)

print("Experiment Results Explanations:", experiment_results_explanations)